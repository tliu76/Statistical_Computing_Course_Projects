{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/USCbiostats/PM520/blob/main/Lab_4_Optimization_PtII.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# Ain't no mountain high enough, or: Optimization Pt II"],"metadata":{"id":"D7LCHBjOhm0i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qP4Zigt0zfMW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738953450010,"user_tz":480,"elapsed":30495,"user":{"displayName":"Tanxin Liu","userId":"08743972115540448245"}},"outputId":"eac4ab29-41e6-403e-d9e0-475a6f5fd8bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lineax\n","  Downloading lineax-0.0.7-py3-none-any.whl.metadata (17 kB)\n","Collecting equinox>=0.11.5 (from lineax)\n","  Downloading equinox-0.11.11-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: jax>=0.4.26 in /usr/local/lib/python3.11/dist-packages (from lineax) (0.4.33)\n","Collecting jaxtyping>=0.2.20 (from lineax)\n","  Downloading jaxtyping-0.2.37-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from lineax) (4.12.2)\n","Collecting jax>=0.4.26 (from lineax)\n","  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n","Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.4.26->lineax)\n","  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (0.4.1)\n","Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (1.26.4)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (3.4.0)\n","Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.26->lineax) (1.13.1)\n","Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.20->lineax)\n","  Downloading wadler_lindig-0.1.3-py3-none-any.whl.metadata (17 kB)\n","Downloading lineax-0.0.7-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading equinox-0.11.11-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jax-0.5.0-py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jaxtyping-0.2.37-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl (102.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m102.0/102.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n","Installing collected packages: wadler-lindig, jaxtyping, jaxlib, jax, equinox, lineax\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.4.33\n","    Uninstalling jaxlib-0.4.33:\n","      Successfully uninstalled jaxlib-0.4.33\n","  Attempting uninstall: jax\n","    Found existing installation: jax 0.4.33\n","    Uninstalling jax-0.4.33:\n","      Successfully uninstalled jax-0.4.33\n","Successfully installed equinox-0.11.11 jax-0.5.0 jaxlib-0.5.0 jaxtyping-0.2.37 lineax-0.0.7 wadler-lindig-0.1.3\n"]}],"source":["!pip install lineax"]},{"cell_type":"markdown","source":["## Gradient Descent Redux\n","Recall under [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) we can iteratively optimize a function $f(\\beta)$ by taking steps in the steepest direction,\n","$$ \\hat{\\beta} = \\beta_t - \\rho_t \\nabla f(\\beta_t).$$\n","\n","A helpful way to recast gradient descent is that we seek to perform a series of _local_ optimizations,\n","\n","$$\\hat{\\beta} = \\min_\\beta \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t}\\|\\beta - \\beta_t\\|_2^2.$$\n","\n","To see how these are equivalent let's solve the local problem. but using inner product notation,\n","$$m(\\beta) = \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t} (\\beta - \\beta_t)^T(\\beta - \\beta_t).$$\n","Now, using calculus again,\n","$$\\begin{align*}\n","\\nabla m(\\beta) &= \\nabla [ \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t} (\\beta - \\beta_t)^T(\\beta - \\beta_t)] \\\\\n","&= \\nabla [\\nabla f(\\beta_t)^T \\beta] + \\frac{1}{2\\rho_t} \\nabla [(\\beta - \\beta_t)^T(\\beta - \\beta_t)] \\\\\n","&= \\nabla f(\\beta_t) + \\frac{1}{\\rho_t}(\\beta - \\beta_t) \\Rightarrow \\\\\n","\\hat{\\beta} &= \\beta_t - \\rho_t \\nabla f(\\beta_t).\n","\\end{align*}\n","$$\n","\n","Neat! However, notice that the original local objective can be thought of as minimizing the directional derivative, but with a distance penalty, where that distance is defined by the geometry of the parameter space.\n","\n","$$\\hat{\\beta} = \\min_\\beta \\nabla f(\\beta_t)^T \\beta + \\frac{1}{2\\rho_t}\\text{dist}(\\beta, \\beta_t).$$\n","\n","When the natural geometry is $\\mathbb{R}^p$ then $\\text{dist}(\\cdot) = \\| \\cdot \\|_2^2$, however there are many  geometries that can describe the natural parameter space (for future class üòâ)"],"metadata":{"id":"10ewnuinXmD0"}},{"cell_type":"markdown","source":["## Newton's Method for Optimization\n","Can we do better, by considering higher-order information (ie geometry) of\n","the function $f$?\n","\n","Let's consider a 2nd-order [Taylor-series approximation](https://en.wikipedia.org/wiki/Taylor_series) to $f$ around $\\beta_t$ as,\n","\n","$$f(\\beta) \\approx f(\\beta_t) + \\nabla f(\\beta_t)^T (\\beta - \\beta_t) + \\frac{1}{2} (\\beta - \\beta_t)^T H(\\beta_t)(\\beta - \\beta_t),$$ where $H(\\beta_t) = \\nabla^2 f(\\beta_t)$ (i.e. the [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix) of $f$ at $\\beta_t$). If we minimize this _local_ approximation, we see\n","\n","$\\nabla_\\beta f(\\beta) \\approx \\nabla f(\\beta_t) + H(\\beta_t)(\\beta - \\beta_t) = \\nabla f(\\beta_t) + H(\\beta_t)\\beta - H(\\beta_t)\\beta_t ‚áí$\n","$$ H(\\beta_t)\\beta = H(\\beta_t)\\beta_t - \\nabla f(\\beta_t).$$\n","\n","We can recognize that this is a [system of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) $A x = b$ where $A = H(\\beta_t)$, $x = \\beta$, and $b = H(\\beta_t)\\beta_t - \\nabla f(\\beta_t)$. The solution is given by, $\\hat{x} = A^{-1}b$, which in this case implies,\n","$$ \\hat{\\beta} = H(\\beta_t)^{-1}\\left(H(\\beta_t)\\beta_t - \\nabla f(\\beta_t)\\right) = \\beta_t - H(\\beta_t)^{-1}\\nabla f(\\beta_t).$$\n","\n","\n","\n","[Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) is only guaranteed to converge _locally_, and can diverge even for _strongly_ [convex functions](https://en.wikipedia.org/wiki/Convex_function) (e.g., $f(\\beta) = \\sqrt{\\beta^2 + 1}$). To address this limitation, we can add a dampening parameter, $\\rho_t$, which gives us our final update form,\n","$$ \\hat{\\beta} = H(\\beta_t)^{-1}(H(\\beta_t)\\beta_t - \\nabla f(\\beta_t)) = \\beta_t - \\rho_t H(\\beta_t)^{-1}\\nabla f(\\beta_t).$$\n","\n","## Quasi-Newton Methods for Optimization\n","What if computing $H(\\beta_t)$ is prohibitive or too costly? Do we need _exact_ second order information to improve on gradient descent's convergence? Given an approximation of $H$, called $B$, i.e. $B(\\beta_t) \\approx H(\\beta_t)$, [_quasi_-Newton methods](https://en.wikipedia.org/wiki/Quasi-Newton_method) optimize for the form\n","$$f(\\beta) \\approx f(\\beta_t) + \\nabla f(\\beta_t)^T (\\beta - \\beta_t) + \\frac{1}{2} (\\beta - \\beta_t)^T B(\\beta_t)(\\beta - \\beta_t),$$ where $B(\\beta_t) \\approx H(\\beta_t)$. Optimizing this statement gives us our update rule,\n","$$ \\hat{\\beta} = \\beta_t - \\rho_t B(\\beta_t)^{-1}\\nabla f(\\beta_t).$$"],"metadata":{"id":"_b5I2Q37z4_1"}},{"cell_type":"markdown","source":["## Poisson Regression\n","\n","$$y_i | x_i \\sim \\text{Poi}(\\lambda_i)$$ where $\\lambda_i := \\exp(x_i^T \\beta)$, and $\\text{Poi}(k | \\lambda) := \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$ is the [PMF](https://en.wikipedia.org/wiki/Probability_mass_function) of the [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution). Given $\\{(y_i, x_i)\\}_{i=1}^n$, we would like to identify the [maximum likelihood parameter estimate](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) for $\\beta$. In other words, we would to find a value for $\\beta$ such that we maximize the log-likelihood given by,\n","$$\\begin{align*}\n","\\log \\ell(\\beta) &= \\sum_i \\log \\text{Poi}(y_i | \\exp(x_i^T \\beta)) \\\\\n","&= \\sum_i \\log \\left[ \\frac{\\exp(y_i \\cdot x_i^T \\beta) \\exp(-\\exp(x_i^T \\beta))}{y_i!} \\right] \\\\\n","&= \\sum_i \\log \\left[ \\frac{\\exp(y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta))}{y_i!} \\right] \\\\\n","&= \\sum_i \\log \\left[\\exp(y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta))\\right] - \\log(y_i!) \\\\\n","&= \\sum_i \\left[y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta) - \\log(y_i!)\\right] \\\\\n","&= y^T X\\beta - \\exp(X\\beta)^T 1_n - O(1) \\\\\n","&= y^T X\\beta - \\lambda^T 1_n - O(1),\n","\\end{align*}$$\n","where $\\lambda = \\{\\lambda_1, \\dotsc, \\lambda_n\\}.$\n","\n","\n","$$ \\begin{align*}\n","\\nabla_\\beta \\ell &= \\nabla_\\beta \\left[ y^T X\\beta - \\lambda^T 1_n \\right] \\\\\n","&= \\nabla_\\beta [ y^T X\\beta ] - \\nabla_\\beta [\\lambda^T 1_n] \\\\\n","&= \\nabla_\\beta [ y^T X\\beta ] - \\nabla_\\beta [\\exp(X\\beta)^T 1_n] \\\\\n","&= X^T y - X^T \\exp(X\\beta)  \\\\\n","&= X^T y - X^T \\lambda  \\\\\n","&= X^T(y - \\lambda) \\\\\n","\\nabla^2_{\\beta \\beta} \\ell &= \\nabla_{\\beta} X^T(y - \\lambda) \\\\\n","&= \\nabla_{\\beta} \\left[X^T y - X^T \\lambda \\right] \\\\\n","&= - X^T \\nabla_{\\beta}  \\lambda \\\\\n","&= -X^T \\nabla_{\\beta}  \\exp(X\\beta) \\\\\n","&= -X^T \\Lambda X,\n","\\end{align*}$$\n","where $\\Lambda = \\text{diag}(\\lambda)$, i.e. $\\Lambda_{ii} = \\lambda_i$ and $\\Lambda_{ij} = 0$ for $i \\neq j$.\n","\n","To illustrate how $\\nabla_{\\beta}  \\exp(X\\beta) = \\Lambda X$ (i.e. last step in Hessian calculation), recall that the [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) of a function $f : \\mathbb{R}^n ‚Üí \\mathbb{R}^m$ is the $m \\times n$ matrix $J$ such that $J_{ij} = \\frac{‚àÇf_i}{‚àÇj}$. In this case we are computing the Jacobian for $\\exp(X\\beta)$, which is $\\mathbb{R}^p ‚Üí \\mathbb{R}^n$, so our final Jacobian for $\\exp(X\\beta)$ should have shape $n \\times p$. Notice that $J_{i,j} = \\frac{\\partial}{\\partial \\beta_j} \\exp(x_i^T \\beta) = x_{ij}\\exp(x_i^T \\beta)$, thus $J_{i, .} = \\exp(x_i^T \\beta) x_i^T$. Repeating this for each $i$ we have $$‚àá_\\beta \\exp(X \\beta) = J(\\exp(X \\beta)) = \\begin{bmatrix} J_{1,.} \\\\ ‚ãÆ \\\\ J_{n,.} \\end{bmatrix} =\n","\\begin{bmatrix} \\exp(x_1^T \\beta) x_1^T \\\\ ‚ãÆ \\\\ \\exp(x_n^T \\beta) x_n^T \\end{bmatrix}  =\n","\\begin{bmatrix} \\lambda_1 x_1^T \\\\ ‚ãÆ \\\\ \\lambda_n x_n^T\\end{bmatrix} = \\Lambda X.$$\n","\n","We can fit using Newton's method. =>\n","$$\\begin{align*}\n","\\beta(t+1) &= \\beta(t) - H(\\beta(t))^{-1}\\nabla \\ell(\\beta_t) \\\\\n","&= \\beta(t) + (X^T \\Lambda(t) X)^{-1} X^T (y - \\lambda) ‚áí \\\\\n","&= (X^T \\Lambda(t) X)^{-1} X^T \\Lambda(t) (\\Lambda(t)^{-1}y + X\\beta(t) - 1)\n","\\end{align*}$$\n","where $\\Lambda(t) := \\text{diag}(\\lambda_1, \\dotsc, \\lambda_n)$."],"metadata":{"id":"crQSiZATzm9D"}},{"cell_type":"code","source":["https://course.ece.cmu.edu/~ece739/lectures/18739-2020-spring-lecture-08-second-order.pdf\n","\n","https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture27.pdf\n"],"metadata":{"id":"5DOc80f6xDIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","import jax.random as rdm\n","import jax.scipy.stats as stats\n","\n","import lineax as lx\n","\n","\n","@jax.jit\n","def loglikelihood(beta, y, X):\n","  \"\"\"\n","  Our loglikelihood function for $y_i | x_i ~ \\text{Poi}(\\exp(eta_i))$.\n","\n","  beta: beta\n","  y: poisson-distributed observations\n","  X: our design matrix\n","\n","  returns: sum of the logliklihoods of each sample\n","  \"\"\"\n","   #loglikelihood function#\n","  eta = X.mv(beta)\n","\n","  #ùë¶ùëáùëãùõΩ‚àíùúÜùëá1ùëõ\n","\n","  return  y @ eta - jnp.sum(jax.numpy.exp(eta))\n","\n","@jax.jit\n","def irwls_fit(beta, y, X, step_size):\n","  \"\"\"\n","  Perform MLE estimation for $\\beta$ under the model\n","     $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n","\n","  beta: beta\n","  y: poisson-distributed observations\n","  X: our design matrix\n","\n","  returns: updated estimate of $\\beta$\n","  \"\"\"\n","\n","\n","  # compute lambda_i := exp(x_i @ beta)\n","  eta = X.mv(beta)\n","  d_i = jnp.exp(eta)\n","  d_sqrt = jnp.sqrt(d_i)\n","\n","  # compute z_i := Lambda^{1/2}(Lambda^-1 y + X @beta - 1)\n","  z = (y / d_i + eta - 1) * d_sqrt\n","\n","  # X* := Lambda^{1/2} X\n","  # we use linear operators to postpone any computation\n","  X_star = lx.DiagonalLinearOperator(d_sqrt) @ X\n","\n","  # lineax can solve normal equations iteratively as (t(X*) @ (X* @ guess)) - z\n","  solution = lx.linear_solve(X_star, z, solver=lx.NormalCG(atol=1e-4, rtol=1e-3))\n","  beta = solution.value\n","\n","  return beta\n","\n","\n","def poiss_reg(y, X, fit_func, step_size = 1.0, max_iter=100, tol=1e-3):\n","  \"\"\"\n","  Perform MLE estimation for $\\beta$ under the model\n","     $y_i | x_i ~ \\text{Poi}(\\exp(x_i^T \\beta))$.\n","\n","  y: poisson-distributed observations\n","  X: our design matrix\n","  max_iter: the maximum number of iterations to perform optimization\n","  tol:\n","\n","  returns: updated estimate of $\\beta$\n","  \"\"\"\n","  # intialize eta := X @ beta\n","  n, p = X.shape\n","\n","  # fake bookkeeping\n","  loglike = -100000\n","  delta = 10000\n","\n","  # convert to a linear operator for lineax\n","  X = lx.MatrixLinearOperator(X)\n","\n","  # initialize using OLS estimate\n","  sol = lx.linear_solve(X, (y - jnp.mean(y))/2, solver=lx.NormalCG(atol=1e-4, rtol=1e-3))\n","  beta = sol.value\n","  beta = beta / jnp.linalg.norm(beta)\n","  for epoch in range(max_iter):\n","\n","    # fit using our function\n","    beta = fit_func(beta, y, X, step_size)\n","\n","    # evaluate log likelihood\n","    newll = loglikelihood(beta, y, X)\n","\n","    # take delta and check if we can stop\n","    delta = jnp.fabs(newll - loglike)\n","    print(f\"Epoch[{epoch}] = {newll}\")\n","    if delta < tol:\n","      break\n","\n","    # replace old value\n","    loglike = newll\n","\n","  return beta"],"metadata":{"id":"DKK1oqZMztkU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P7JxbEa_5ivu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","import jax.numpy.linalg as jnpla\n","import jax.random as rdm\n","import jax.scipy.linalg as jspla\n","import lineax as lx\n","# Let's simulate a poisson regression model with N samples and P variables\n","# X: (N, P) , beta: (P,), and y: (N,)\n","N = 1000\n","P = 5\n","\n","# initialize PRNG env\n","seed = 0\n","key = rdm.PRNGKey(seed)\n","\n","# TODO: split key for each random call\n","\n","key, x_key = rdm.split(key)\n","X = rdm.normal(x_key, shape=(N, P))\n","key, b_key = rdm.split(key)\n","beta = rdm.normal(b_key, shape=(P,))\n","\n","# TODO: compute lambda_i\n","lamba = jax.numpy.exp(X @ beta)\n","\n","# TODO: sample y from Poi(lambda_i)\n","k=3\n","#y = jax.scipy.stats.poisson.pmf(k, lamba)\n","y = rdm.poisson(b_key, lamba,(N,))\n","\n","# estimate beta using our irwls function\n","step_size =0.001\n","X.op = lx.MatrixLinearOperator(X)\n","beta = irwls_fit(beta, y, X.op, step_size)\n","\n","# fit_func has signature (eta, y, X, step_size)\n","beta_hat = poiss_reg(y, X, irwls_fit,step_size)\n","print(f\"beta = {beta}\")\n","print(f\"hat(beta) = {beta_hat}\")"],"metadata":{"id":"nRai4qWiz1_R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738958995026,"user_tz":480,"elapsed":574,"user":{"displayName":"Tanxin Liu","userId":"08743972115540448245"}},"outputId":"0c7b5495-4cb3-454d-c676-28776a7ecb23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch[0] = -1414727040.0\n","Epoch[1] = -522715776.0\n","Epoch[2] = -193111680.0\n","Epoch[3] = -71456312.0\n","Epoch[4] = -26475514.0\n","Epoch[5] = -9813868.0\n","Epoch[6] = -3628856.25\n","Epoch[7] = -1333594.125\n","Epoch[8] = -472925.78125\n","Epoch[9] = -151409.265625\n","Epoch[10] = -31631.16015625\n","Epoch[11] = 11638.0078125\n","Epoch[12] = 25922.361328125\n","Epoch[13] = 29637.5078125\n","Epoch[14] = 30175.57421875\n","Epoch[15] = 30195.9375\n","Epoch[16] = 30195.994140625\n","Epoch[17] = 30196.01953125\n","Epoch[18] = 30196.0\n","Epoch[19] = 30196.0078125\n","Epoch[20] = 30195.984375\n","Epoch[21] = 30196.0078125\n","Epoch[22] = 30196.00390625\n","Epoch[23] = 30196.001953125\n","Epoch[24] = 30195.98828125\n","Epoch[25] = 30196.01171875\n","Epoch[26] = 30196.00390625\n","Epoch[27] = 30196.0\n","Epoch[28] = 30196.001953125\n","Epoch[29] = 30196.0234375\n","Epoch[30] = 30195.9921875\n","Epoch[31] = 30196.001953125\n","Epoch[32] = 30196.0\n","Epoch[33] = 30196.01171875\n","Epoch[34] = 30196.013671875\n","Epoch[35] = 30195.998046875\n","Epoch[36] = 30196.03125\n","Epoch[37] = 30195.998046875\n","Epoch[38] = 30195.990234375\n","Epoch[39] = 30196.021484375\n","Epoch[40] = 30196.009765625\n","Epoch[41] = 30196.021484375\n","Epoch[42] = 30196.01953125\n","Epoch[43] = 30196.021484375\n","Epoch[44] = 30196.005859375\n","Epoch[45] = 30196.00390625\n","Epoch[46] = 30196.017578125\n","Epoch[47] = 30196.001953125\n","Epoch[48] = 30195.9921875\n","Epoch[49] = 30196.009765625\n","Epoch[50] = 30196.001953125\n","Epoch[51] = 30196.01171875\n","Epoch[52] = 30195.998046875\n","Epoch[53] = 30196.0234375\n","Epoch[54] = 30196.00390625\n","Epoch[55] = 30196.01953125\n","Epoch[56] = 30196.005859375\n","Epoch[57] = 30196.015625\n","Epoch[58] = 30196.0\n","Epoch[59] = 30196.01953125\n","Epoch[60] = 30196.01171875\n","Epoch[61] = 30196.0078125\n","Epoch[62] = 30195.9921875\n","Epoch[63] = 30196.009765625\n","Epoch[64] = 30196.00390625\n","Epoch[65] = 30195.990234375\n","Epoch[66] = 30196.00390625\n","Epoch[67] = 30196.021484375\n","Epoch[68] = 30196.013671875\n","Epoch[69] = 30196.015625\n","Epoch[70] = 30196.00390625\n","Epoch[71] = 30196.013671875\n","Epoch[72] = 30196.001953125\n","Epoch[73] = 30196.015625\n","Epoch[74] = 30196.015625\n","beta = [-1.2472982  -0.42615604 -1.1338782   0.8631554  -0.84650266]\n","hat(beta) = [-1.247062   -0.42620674 -1.1337787   0.862929   -0.8461564 ]\n"]}]},{"cell_type":"code","source":["# let's implement poisson regression using _only_ gradient information to perform inference\n","# and measure how quickly it converges compared with the Newton method\n","def grad_fit(beta, y, X, step_size):\n","  eta = X.mv(beta)\n","\n","  grad = X.mv(y - X @ beta)\n","  pass\n","\n","# NB: we can transpose a lx.MatrixLinearOperator (say X) as X.transpose()\n","# NB: we compute matrix-vector produces using a lx.MatrixLinearOperator as X.mv(v)\n","step_size = 1e-7\n","beta_hat = poiss_reg(y, X, grad_fit, step_size, max_iter=1000)\n","print(f\"beta = {beta}\")\n","print(f\"hat(beta) = {beta_hat}\")"],"metadata":{"id":"ZalxS2NOOfiV","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1739521761349,"user_tz":480,"elapsed":221,"user":{"displayName":"Tanxin Liu","userId":"08743972115540448245"}},"outputId":"dac3e769-62f8-433e-dc52-b7cec961d8dd"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'poiss_reg' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a618e9ef4baf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# NB: we compute matrix-vector produces using a lx.MatrixLinearOperator as X.mv(v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mbeta_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoiss_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"beta = {beta}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"hat(beta) = {beta_hat}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'poiss_reg' is not defined"]}]},{"cell_type":"markdown","source":["## Automatic differentiation\n","Chain rules, okay! Notes TBD"],"metadata":{"id":"WbJ1nlWE0R7T"}},{"cell_type":"code","source":["# let's not worry and use autodiff\n","#autodiff reverse mode differention\n","\n","\n","auto_grad_ll = jax.grad(loglikelihood)\n","\n","def jax_grad_step(beta, y, X, step_size):\n","  pass\n","\n","# NB: we can transpose a lx.MatrixLinearOperator (say X) as X.transpose()\n","# NB: we compute matrix-vector produces using a lx.MatrixLinearOperator as X.mv(v)\n","step_size = 1e-7\n","beta_hat = poiss_reg(y, X, jax_grad_step, step_size, max_iter=1000)\n","print(f\"beta = {beta}\")\n","print(f\"hat(beta) = {beta_hat}\")"],"metadata":{"id":"dSPl_smqUq9y","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1739521830865,"user_tz":480,"elapsed":188,"user":{"displayName":"Tanxin Liu","userId":"08743972115540448245"}},"outputId":"6abef057-b526-4d70-b5db-8d74e92afaf3"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'jax' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-964374114d88>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mauto_grad_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglikelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjax_grad_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'jax' is not defined"]}]},{"cell_type":"code","source":["import jax.scipy.linalg as spla\n","\n","# Great! But can we use 2nd order information?\n","auto_hess_ll = jax.hessian(loglikelihood)\n","\n","def jax_newton_step(beta, y, X, step_size):\n","  grad = auto_grad_ll(beta,y,X)\n","  return beta + step_size + grad\n","# NB: we can transpose a lx.MatrixLinearOperator (say X) as X.transpose()\n","# NB: we compute matrix-vector produces using a lx.MatrixLinearOperator as X.mv(v)\n","step_size = 1.\n","beta_hat = poiss_reg(y, X, jax_newton_step, step_size, max_iter=1000)\n","print(f\"beta = {beta}\")\n","print(f\"hat(beta) = {beta_hat}\")"],"metadata":{"id":"qtbqZkAlWPuv","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1739521828666,"user_tz":480,"elapsed":672,"user":{"displayName":"Tanxin Liu","userId":"08743972115540448245"}},"outputId":"4621f070-9242-4858-fc4d-bf9441c563aa"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'jax' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ab1b27c9fd5a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Great! But can we use 2nd order information?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mauto_hess_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglikelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjax_newton_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'jax' is not defined"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"DiAxKhU6Bux3"}},{"cell_type":"markdown","source":[],"metadata":{"id":"7wum8codBuz6"}},{"cell_type":"markdown","source":[],"metadata":{"id":"zVhlDLRLBu1_"}},{"cell_type":"markdown","source":["\n","\n"," **other**  \n","we would to find a value for $\\beta$ such that we maximize the log-likelihood given by,\n","$$\\begin{align*}\n","\\log \\ell(\\beta) &= \\sum_i \\log \\text{Poi}(y_i | \\exp(x_i^T \\beta)) \\\\\n","&= \\sum_i \\log \\left[ \\frac{\\exp(y_i \\cdot x_i^T \\beta) \\exp(-\\exp(x_i^T \\beta))}{y_i!} \\right] \\\\\n","&= \\sum_i \\log \\left[ \\frac{\\exp(y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta))}{y_i!} \\right] \\\\\n","&= \\sum_i \\log \\left[\\exp(y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta))\\right] - \\log(y_i!) \\\\\n","&= \\sum_i \\left[y_i \\cdot x_i^T \\beta - \\exp(x_i^T \\beta) - \\log(y_i!)\\right] \\\\\n","&= y^T X\\beta - \\exp(X\\beta)^T 1_n - O(1) \\\\\n","&= y^T X\\beta - \\lambda^T 1_n - O(1),\n","\\end{align*}$$\n","where $\\lambda = \\{\\lambda_1, \\dotsc, \\lambda_n\\}.$\n","\n","##### Derive the $ \\hat{\\beta} $ :\n","\n","First-order gradient:\n","$$ \\begin{align*}\n","\\nabla_\\beta \\ell &= \\nabla_\\beta \\left[ y^T X\\beta - \\lambda^T 1_n \\right] \\\\\n","&= \\nabla_\\beta [ y^T X\\beta ] - \\nabla_\\beta [\\lambda^T 1_n] \\\\\n","&= \\nabla_\\beta [ y^T X\\beta ] - \\nabla_\\beta [\\exp(X\\beta)^T 1_n] \\\\\n","&= X^T y - \\exp(X\\beta)^T X  \\\\\n","&= X^T y - \\lambda^T X  \\\\\n","&= X^T(y - \\lambda) \\\\\n","\\end{align*}$$\n","Second -order gradients:  \n","$$ \\begin{align*}\n","\\nabla^2_{\\beta \\beta} \\ell &= \\nabla_{\\beta} X^T(y - \\lambda) \\\\\n","&= \\nabla_{\\beta} \\left[X^T y - X^T \\lambda \\right] \\\\\n","&= - X^T \\nabla_{\\beta}  \\lambda \\\\\n","&= -X^T \\nabla_{\\beta}  \\exp(X\\beta) \\\\\n","&= -X^T \\Lambda X,\n","\\end{align*}$$\n","\n","where $\\Lambda = \\text{diag}(\\lambda)$, i.e. $\\Lambda_{ii} = \\lambda_i$ and $\\Lambda_{ij} = 0$ for $i \\neq j$.\n","\n","Newton's method:\n","$$\\begin{align*}\n","\\beta(t+1) &= \\beta(t) - H(\\beta(t))^{-1}\\nabla \\ell(\\beta_t) \\\\\n","&= \\beta(t) + (X^T \\Lambda(t) X)^{-1} X^T (y - \\lambda) ‚áí \\\\\n","&= (X^T \\Lambda(t) X)^{-1} X^T \\Lambda(t) (\\Lambda(t)^{-1}y + X\\beta(t) - 1)\n","\\end{align*}$$\n","where $\\Lambda(t) := \\text{diag}(\\lambda_1, \\dotsc, \\lambda_n)$.\n","\n","\n","\n"],"metadata":{"id":"X4bsqSoWBu4n"}}]}